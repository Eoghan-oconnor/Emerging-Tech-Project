{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "## This notebook will contain the model and all explaintions.\n",
    "Please refer to the README for instructuins on how to run the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "# NumPy is a library for the Python\n",
    "# programming language, adding support for large,\n",
    "# multi-dimensional arrays and matrices\n",
    "import numpy as np\n",
    "# Matplot lib is a 2D plotting library for Python\n",
    "import matplotlib.pyplot as plt\n",
    "# Keras is a hign level open-source nerual-network library written in Python\n",
    "# it runs on top of TensorFlow, Theano and PlaidML\n",
    "import keras as kr\n",
    "# The MNIST database is a large database of handwritten digits that is\n",
    "# commonly used for training various image processing systems.\n",
    "# This imports The MNIST dirctly from the from the keras API\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "# Importing the constants.py\n",
    "import constants\n",
    "# Importing from keras Dense which implemnets the operation, Flatten is an\n",
    "# operation preformed on a tensor that reshapes the tensor to have a\n",
    "# shape that is equal to the number of elements contained in tensor.\n",
    "# Conv2D converts the image into pixels and takes an n-sized window\n",
    "# those features are then condensed into a feature map and the\n",
    "# window slides\n",
    "# MaxPooling2D is used for spatial data\n",
    "# Dropout is a feature that stops certain neurals from training in order to\n",
    "# prevent an overfitting\n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout\n",
    "\n",
    "from keras.models import load_model\n",
    "# https://machinelearningmastery.com/save-load-keras-deep-learning-models/\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This downloads the MNIST dataset from the Keras API. The dataset has 60,000\n",
    "# images and associated labels used for training and 10,000 testing images\n",
    "# with associated labels. We need to seperate the dataset into two groups\n",
    "# a training group and a testing group. train_imgs, train_labels,\n",
    "# test_imgs and test_labels.\n",
    "(train_imgs, train_labels), (test_imgs, test_labels) = mnist.load_data()\n",
    "\n",
    "# Debug\n",
    "# print(train_imgs.shape[0])\n",
    "# print(train_labels.shape[0])\n",
    "\n",
    "# print(test_imgs.shape[0])\n",
    "# print(test_labels.shape[0])\n",
    "\n",
    "# Setting image width and height to 28.\n",
    "img_height, img_width = 28, 28\n",
    "\n",
    "# We have to reshape the MNIST dataset with Keras, we will convert it from a 3d\n",
    "# Array to a 4d NumPy array\n",
    "# Making sure train_imgs and test_imgs are floats, so we can use decimal points\n",
    "train_imgs = train_imgs.reshape(train_imgs.shape[0], img_width,\n",
    "                                img_height, 1)\n",
    "test_imgs = test_imgs.reshape(test_imgs.shape[0], img_width,\n",
    "                              img_height, 1)\n",
    "input_shape = (img_width, img_height, 1)\n",
    "train_imgs = train_imgs.astype('float32')\n",
    "test_imgs = test_imgs.astype('float32')\n",
    "\n",
    "# Data is normalized when being used in a nerual network to obtain a mean close\n",
    "# to 0 Normalizing the data generally speeds up learning and\n",
    "# leads to faster convergence. Normalizing the RGB code by dividing it by the\n",
    "# max RGB value(255)\n",
    "train_imgs /= 255\n",
    "test_imgs /= 255\n",
    "\n",
    "# There are ten numbers (0-9) so the Output layer our NN only need ten layers. \n",
    "num_classes = 10\n",
    "# Assigns labels to associated images \n",
    "\n",
    "train_labels = kr.utils.to_categorical(train_labels, num_classes)\n",
    "test_labels = kr.utils.to_categorical(test_labels, num_classes)\n",
    "train_labels[0]\n",
    "\n",
    "# Debug\n",
    "# print(test_imgs.shape[0])\n",
    "# print(test_labels.shape[0])\n",
    "\n",
    "# Creating a model and adding layers\n",
    "# Sequential allows you to create a nerual network layer by layer\n",
    "# Conv2D, MaxPooling2D, Dropout, and Flatten all explained above\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu', input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(constants.rate))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.compile(loss=kr.losses.categorical_crossentropy,\n",
    "              optimizer=kr.optimizers.Adadelta(), metrics=['accuracy'])\n",
    "\n",
    "# Current set up requires the model to be trained every time the application\n",
    "# is ran, so we will train out file here and ave the model for future use.\n",
    "# The try and except block in Python is used to catch and handle exceptions.\n",
    "# Python executes code following the try statement as a “normal” part of the\n",
    "# program.The code that follows the except statement is the program's response\n",
    "# to any exceptions in the preceding try clause.\n",
    "\n",
    "# Debug\n",
    "# print(test_imgs.shape)\n",
    "# print(test_labels.shape)\n",
    "\n",
    "# Also converting the model to a json file.\n",
    "\n",
    "model.fit(train_imgs, train_labels,\n",
    "          batch_size=constants.batch_size,\n",
    "          epochs=constants.num_epoch,\n",
    "          verbose=1,\n",
    "          validation_data=(test_imgs, test_labels)\n",
    "          )\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights(\"model.h5\")\n",
    "model.save(\"model.h5\")\n",
    "\n",
    "plt.imshow(test_imgs[3333].reshape(28, 28), cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "try:\n",
    "    print(\"Model loaded\")\n",
    "    model = load_model(\"model.h5\")\n",
    "\n",
    "    # Adding json version for use in flask server\n",
    "    json_file = open('model.json', 'r')\n",
    "    loaded_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_json)\n",
    "\n",
    "except IndexError:\n",
    "    print(\"Error opening file, no model Present\")\n",
    "    print(\"Creating Model\")\n",
    "    model_log = model.fit(train_imgs, train_labels,\n",
    "                          batch_size=constants.batch_size,\n",
    "                          epochs=constants.num_epoch,\n",
    "                          verbose=1,\n",
    "                          validation_data=(test_imgs, test_labels))\n",
    "   \n",
    "    model.save_weights(\"model.h5\")\n",
    "    model.save(\"model.h5\")\n",
    "    print(\"Model Saved.\")\n",
    "\n",
    "plt.imshow(test_imgs[3333].reshape(28, 28), cmap=\"gray\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
